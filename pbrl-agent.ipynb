{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preference-based Reinforcement learning\n",
    "## Configuring an agent\n",
    "We will first define an agent using our PbRL-framework. An agent needs functionality to:\n",
    "- generate query candidates\n",
    "- select queries\n",
    "- present queries and collect preferences\n",
    "- train the reward model\n",
    "\n",
    "For this, we mix concrete implementations into an abstract agent shell via multiple\n",
    "inheritance and specify the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from agent.preference_based.sequential.sequential_pbrl_agent import AbstractSequentialPbRLAgent\n",
    "\n",
    "from preference_data.query_generation.segment.segment_query_generator import RandomSegmentQueryGenerator\n",
    "from preference_data.query_selection.query_selector import IndexQuerySelector\n",
    "from preference_data.querent.synchronous.oracle.oracle import RewardMaximizingOracle\n",
    "from reward_modeling.reward_trainer import RewardTrainer\n",
    "from wrappers.utils import create_env\n",
    "\n",
    "class SequentialPbRLAgent(AbstractSequentialPbRLAgent, RandomSegmentQueryGenerator, IndexQuerySelector,\n",
    "                          RewardMaximizingOracle, RewardTrainer):\n",
    "    def __init__(self, env, num_pretraining_epochs=10, num_training_epochs_per_iteration=10,\n",
    "                 preferences_per_iteration=500):\n",
    "        AbstractSequentialPbRLAgent.__init__(self, env,\n",
    "                                             num_pretraining_epochs=num_pretraining_epochs,\n",
    "                                             num_training_epochs_per_iteration=num_training_epochs_per_iteration,\n",
    "                                             preferences_per_iteration=preferences_per_iteration)\n",
    "        RandomSegmentQueryGenerator.__init__(self, self.policy_model, segment_sampling_interval=50)\n",
    "        RewardTrainer.__init__(self, self.reward_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent\n",
    "We are now ready to train the agent. The agent will:\n",
    " - conduct reinforcement learning in the given environment,\n",
    " - thereby generating trajectories\n",
    " - that will be combined into queries and posed to an oracle,\n",
    " - which are then used to train the reward model.\n",
    "The whole process ends once the agent ran 200,000 timesteps in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Trajectory segment sampling failed. Fewer elements in buffer than sample size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reward model pretraining\n",
      "Pretraining: Start data collection\n",
      "Pretraining: Start reward model training\n",
      "Start reward model training\n",
      "Training: Start new training iteration. 0/200000 (0.00%) RL training steps completed.\n",
      "Training: Start new training iteration. 1950/200000 (0.97%) RL training steps completed.\n",
      "Training: Start new training iteration. 3900/200000 (1.95%) RL training steps completed.\n",
      "Training: Start new training iteration. 5850/200000 (2.93%) RL training steps completed.\n",
      "Training: Start new training iteration. 7800/200000 (3.90%) RL training steps completed.\n",
      "Training: Start new training iteration. 9750/200000 (4.88%) RL training steps completed.\n",
      "Training: Start new training iteration. 11700/200000 (5.85%) RL training steps completed.\n",
      "Training: Start new training iteration. 13650/200000 (6.83%) RL training steps completed.\n",
      "Training: Start new training iteration. 15600/200000 (7.80%) RL training steps completed.\n",
      "Training: Start new training iteration. 17550/200000 (8.77%) RL training steps completed.\n",
      "Training: Start new training iteration. 19500/200000 (9.75%) RL training steps completed.\n",
      "Training: Start new training iteration. 21450/200000 (10.72%) RL training steps completed.\n",
      "Training: Start new training iteration. 23400/200000 (11.70%) RL training steps completed.\n",
      "Training: Start new training iteration. 25350/200000 (12.68%) RL training steps completed.\n",
      "Training: Start new training iteration. 27300/200000 (13.65%) RL training steps completed.\n",
      "Training: Start new training iteration. 29250/200000 (14.62%) RL training steps completed.\n",
      "Training: Start new training iteration. 31200/200000 (15.60%) RL training steps completed.\n",
      "Training: Start new training iteration. 33150/200000 (16.57%) RL training steps completed.\n",
      "Training: Start new training iteration. 35100/200000 (17.55%) RL training steps completed.\n",
      "Training: Start new training iteration. 37050/200000 (18.52%) RL training steps completed.\n",
      "Training: Start new training iteration. 39000/200000 (19.50%) RL training steps completed.\n",
      "Training: Start new training iteration. 40950/200000 (20.47%) RL training steps completed.\n",
      "Training: Start new training iteration. 42900/200000 (21.45%) RL training steps completed.\n",
      "Training: Start new training iteration. 44850/200000 (22.43%) RL training steps completed.\n",
      "Training: Start new training iteration. 46800/200000 (23.40%) RL training steps completed.\n",
      "Training: Start new training iteration. 48750/200000 (24.38%) RL training steps completed.\n",
      "Training: Start new training iteration. 50700/200000 (25.35%) RL training steps completed.\n",
      "Training: Start new training iteration. 52650/200000 (26.32%) RL training steps completed.\n",
      "Training: Start new training iteration. 54600/200000 (27.30%) RL training steps completed.\n",
      "Training: Start new training iteration. 56550/200000 (28.27%) RL training steps completed.\n",
      "Training: Start new training iteration. 58500/200000 (29.25%) RL training steps completed.\n",
      "Training: Start new training iteration. 60450/200000 (30.23%) RL training steps completed.\n",
      "Training: Start new training iteration. 62400/200000 (31.20%) RL training steps completed.\n",
      "Training: Start new training iteration. 64350/200000 (32.17%) RL training steps completed.\n",
      "Training: Start new training iteration. 66300/200000 (33.15%) RL training steps completed.\n",
      "Training: Start new training iteration. 68250/200000 (34.12%) RL training steps completed.\n",
      "Training: Start new training iteration. 70200/200000 (35.10%) RL training steps completed.\n",
      "Training: Start new training iteration. 72150/200000 (36.08%) RL training steps completed.\n",
      "Training: Start new training iteration. 74100/200000 (37.05%) RL training steps completed.\n",
      "Training: Start new training iteration. 76050/200000 (38.02%) RL training steps completed.\n",
      "Training: Start new training iteration. 78000/200000 (39.00%) RL training steps completed.\n",
      "Training: Start new training iteration. 79950/200000 (39.98%) RL training steps completed.\n",
      "Training: Start new training iteration. 81900/200000 (40.95%) RL training steps completed.\n",
      "Training: Start new training iteration. 83850/200000 (41.93%) RL training steps completed.\n",
      "Training: Start new training iteration. 85800/200000 (42.90%) RL training steps completed.\n",
      "Training: Start new training iteration. 87750/200000 (43.88%) RL training steps completed.\n",
      "Training: Start new training iteration. 89700/200000 (44.85%) RL training steps completed.\n",
      "Training: Start new training iteration. 91650/200000 (45.82%) RL training steps completed.\n",
      "Training: Start new training iteration. 93600/200000 (46.80%) RL training steps completed.\n",
      "Training: Start new training iteration. 95550/200000 (47.77%) RL training steps completed.\n",
      "Training: Start new training iteration. 97500/200000 (48.75%) RL training steps completed.\n",
      "Training: Start new training iteration. 99450/200000 (49.73%) RL training steps completed.\n",
      "Training: Start new training iteration. 101400/200000 (50.70%) RL training steps completed.\n",
      "Training: Start new training iteration. 103350/200000 (51.68%) RL training steps completed.\n",
      "Training: Start new training iteration. 105300/200000 (52.65%) RL training steps completed.\n",
      "Training: Start new training iteration. 107250/200000 (53.62%) RL training steps completed.\n",
      "Training: Start new training iteration. 109200/200000 (54.60%) RL training steps completed.\n",
      "Training: Start new training iteration. 111150/200000 (55.57%) RL training steps completed.\n",
      "Training: Start new training iteration. 113100/200000 (56.55%) RL training steps completed.\n",
      "Training: Start new training iteration. 115050/200000 (57.53%) RL training steps completed.\n",
      "Training: Start new training iteration. 117000/200000 (58.50%) RL training steps completed.\n",
      "Training: Start new training iteration. 118950/200000 (59.48%) RL training steps completed.\n",
      "Training: Start new training iteration. 120900/200000 (60.45%) RL training steps completed.\n",
      "Training: Start new training iteration. 122850/200000 (61.42%) RL training steps completed.\n",
      "Training: Start new training iteration. 124800/200000 (62.40%) RL training steps completed.\n",
      "Training: Start new training iteration. 126750/200000 (63.38%) RL training steps completed.\n",
      "Training: Start new training iteration. 128700/200000 (64.35%) RL training steps completed.\n",
      "Training: Start new training iteration. 130650/200000 (65.33%) RL training steps completed.\n",
      "Training: Start new training iteration. 132600/200000 (66.30%) RL training steps completed.\n",
      "Training: Start new training iteration. 134550/200000 (67.27%) RL training steps completed.\n",
      "Training: Start new training iteration. 136500/200000 (68.25%) RL training steps completed.\n",
      "Training: Start new training iteration. 138450/200000 (69.23%) RL training steps completed.\n",
      "Training: Start new training iteration. 140400/200000 (70.20%) RL training steps completed.\n",
      "Training: Start new training iteration. 142350/200000 (71.17%) RL training steps completed.\n",
      "Training: Start new training iteration. 144300/200000 (72.15%) RL training steps completed.\n",
      "Training: Start new training iteration. 146250/200000 (73.12%) RL training steps completed.\n",
      "Training: Start new training iteration. 148200/200000 (74.10%) RL training steps completed.\n",
      "Training: Start new training iteration. 150150/200000 (75.08%) RL training steps completed.\n",
      "Training: Start new training iteration. 152100/200000 (76.05%) RL training steps completed.\n",
      "Training: Start new training iteration. 154050/200000 (77.03%) RL training steps completed.\n",
      "Training: Start new training iteration. 156000/200000 (78.00%) RL training steps completed.\n",
      "Training: Start new training iteration. 157950/200000 (78.97%) RL training steps completed.\n",
      "Training: Start new training iteration. 159900/200000 (79.95%) RL training steps completed.\n",
      "Training: Start new training iteration. 161850/200000 (80.92%) RL training steps completed.\n",
      "Training: Start new training iteration. 163800/200000 (81.90%) RL training steps completed.\n",
      "Training: Start new training iteration. 165750/200000 (82.88%) RL training steps completed.\n",
      "Training: Start new training iteration. 167700/200000 (83.85%) RL training steps completed.\n",
      "Training: Start new training iteration. 169650/200000 (84.82%) RL training steps completed.\n",
      "Training: Start new training iteration. 171600/200000 (85.80%) RL training steps completed.\n",
      "Training: Start new training iteration. 173550/200000 (86.78%) RL training steps completed.\n",
      "Training: Start new training iteration. 175500/200000 (87.75%) RL training steps completed.\n",
      "Training: Start new training iteration. 177450/200000 (88.72%) RL training steps completed.\n",
      "Training: Start new training iteration. 179400/200000 (89.70%) RL training steps completed.\n",
      "Training: Start new training iteration. 181350/200000 (90.67%) RL training steps completed.\n",
      "Training: Start new training iteration. 183300/200000 (91.65%) RL training steps completed.\n",
      "Training: Start new training iteration. 185250/200000 (92.62%) RL training steps completed.\n",
      "Training: Start new training iteration. 187200/200000 (93.60%) RL training steps completed.\n",
      "Training: Start new training iteration. 189150/200000 (94.58%) RL training steps completed.\n",
      "Training: Start new training iteration. 191100/200000 (95.55%) RL training steps completed.\n",
      "Training: Start new training iteration. 193050/200000 (96.53%) RL training steps completed.\n",
      "Training: Start new training iteration. 195000/200000 (97.50%) RL training steps completed.\n"
     ]
    }
   ],
   "source": [
    "env = create_env(\"MountainCar-v0\", termination_penalty=10.)\n",
    "\n",
    "agent = SequentialPbRLAgent(env=env, num_pretraining_epochs=8,\n",
    "                                num_training_epochs_per_iteration=16,\n",
    "                                preferences_per_iteration=32)\n",
    "\n",
    "agent.learn_reward_model(num_training_timesteps=200000, num_pretraining_preferences=512)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can monitor the learning process via TensorBoard. Start TensorBoard in a terminal:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}